# Output JSON Schema

This document describes the complete structure of the JSON output files generated by the experiments.

## Record Structure

Each experiment generates a JSON file containing an array of records. Each record represents one generated output and contains the following fields:

### Core Identification Fields

| Field | Type | Description |
|-------|------|-------------|
| `timestamp` | string (ISO 8601) | Timestamp when the output was generated |
| `config_name` | string | Name of the configuration profile used |

### Model Information

| Field | Type | Description |
|-------|------|-------------|
| `model_name` | string | Full model identifier (e.g., "google/gemma-3-4b-it") |
| `model_variant` | string | Model variant: always "it" (instruction-tuned). Pretrained variants not supported in this version. |
| `device` | string | Device where the model was loaded (e.g., "cuda:0") |
| `dtype` | string | Tensor data type used: "float16", "float32", or "bfloat16" |
| `load_4bit` | boolean | Whether the model was loaded with 4-bit quantization (for VRAM reduction, distinct from `uni_quant` degradation) |
| `restore_strategy` | string | Restoration strategy used (currently always "subset_in_memory") |

### Degradation Parameters

| Field | Type | Description |
|-------|------|-------------|
| `degradation_method` | string | Method used: "mult_gauss", "ablation", or "uni_quant" |
| `param_group_name` | string | Which component was degraded: "attn_only", "mlp_only", or "embed_only" |
| `level_value` | float | Degradation level control parameter (see note below) |
| `level_index` | integer | Index of this degradation level in the experiment sequence (0-based) |
| `repeat_index` | integer | Repetition number for this degradation level (0-based) |

**Note on control parameter (`level_value`):** 
- For `mult_gauss`: standard deviation of Gaussian noise
- For `ablation`: masking proportion (0.0 to 1.0)
- For `uni_quant`: number of quantization levels

### Prompt Information

| Field | Type | Description |
|-------|------|-------------|
| `prompt_group` | string | Task category (e.g., "dreams", "cookie_theft", "math", "lang") |
| `prompt_id` | integer | Sequential ID of the prompt within its group (1-based) |
| `prompt_text` | string | The actual prompt text sent to the model |

### Generation Results

| Field | Type | Description |
|-------|------|-------------|
| `output` | string | The generated text output from the model |
| `tokens_in` | integer | Number of tokens in the input prompt |
| `tokens_out` | integer | Number of tokens in the generated output |
| `duration` | float | Time taken to generate the output (seconds) |

**Important:** Fields `output` and `duration` retain their original names for backward compatibility. Do not rename them.

### Generation Configuration

| Field | Type | Description |
|-------|------|-------------|
| `gen_params` | object | Dictionary with generation parameters |
| `gen_params.temperature` | float | Sampling temperature |
| `gen_params.do_sample` | boolean | Whether sampling is enabled |
| `gen_params.max_new_tokens` | integer | Maximum tokens to generate |
| `seed` | integer | Random seed used for this generation |
| `batch_size_effective` | integer | Actual batch size used during generation |

**Note:** `top_k` and `top_p` are not currently implemented in the generation parameters, but can be added if needed.

### Image-Related Fields (optional)

| Field | Type | Description |
|-------|------|-------------|
| `image_used` | boolean | Whether an image was provided as input |
| `image_filename` | string or null | Filename of the input image (if used) |

### Performance Metrics (optional)

| Field | Type | Description |
|-------|------|-------------|
| `vram_usage_percent` | float or null | Percentage of VRAM used at generation time |
| `perplexity` | float or null | Model perplexity at this degradation level (only present if `compute_perplexity=True` in config). Calculated once per degradation level, not per repetition. |

### Legacy Fields (maintained for compatibility)

| Field | Type | Description |
|-------|------|-------------|
| `std_dev` | float | Degradation parameter value (superseded by `level_value`) |
| `tokens` | integer | Total number of tokens in the output (superseded by `tokens_out`) |
| `force_run` | boolean | Legacy field from original code. Not implemented in current version - system always resumes from existing results. May be added as optional feature in future. |
| `usar_4bit` | boolean | Whether 4-bit quantization was used (superseded by `load_4bit`) |
| `usar_data_parallel` | boolean | Legacy field from original code. DataParallel is not used in this version. Field removed from new outputs. |

## Example Complete Record

```json
{
  "timestamp": "2025-10-06T12:34:56.789Z",
  "config_name": "dreams_it__quant_attn",
  "model_name": "google/gemma-3-4b-it",
  "model_variant": "it",
  "device": "cuda:0",
  "dtype": "float16",
  "load_4bit": false,
  "restore_strategy": "subset_in_memory",
  "degradation_method": "uni_quant",
  "param_group_name": "attn_only",
  "std_dev": 64,
  "level_value": 64,
  "level_index": 2,
  "repeat_index": 0,
  "prompt_group": "dreams",
  "prompt_id": 1,
  "prompt_text": "Just narrate a dream with as much detail as you can.",
  "output": "I was standing in a vast, misty forest. The trees seemed to stretch endlessly upward, their branches intertwining like ancient fingers. A faint melody echoed through the air, though I couldn't identify its source. As I walked deeper into the woods, I noticed the ground beneath my feet was not solid earth but a soft, glowing moss that pulsed with each step...",
  "tokens": 312,
  "tokens_in": 42,
  "tokens_out": 312,
  "duration": 1.23,
  "gen_params": {
    "temperature": 1.0,
    "do_sample": true,
    "max_new_tokens": 350
  },
  "seed": 42001,
  "batch_size_effective": 9,
  "image_used": false,
  "image_filename": null,
  "vram_usage_percent": 67.3,
  "usar_4bit": false
}
```

## Field Evolution

The schema is designed to be backward-compatible:
- **Original fields** are maintained with their original names and types for backward compatibility
- **New fields** are added incrementally without breaking existing analysis code
- **Deprecated fields** (like `usar_4bit`) are kept for compatibility but superseded by newer equivalents

This ensures that:
- Old notebooks can still read new outputs (ignoring unknown fields)
- New notebooks can read old outputs (using only the fields that exist)
- Migration can happen gradually without breaking existing analysis pipelines

